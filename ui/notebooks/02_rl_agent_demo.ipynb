{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Agent Live Trading Demo\n",
    "\n",
    "This notebook demonstrates live reinforcement learning agents trading in the FinSim simulation environment.\n",
    "\n",
    "## RL Algorithms Implemented\n",
    "- **DQN (Deep Q-Network)**: Value-based learning with experience replay\n",
    "- **PPO (Proximal Policy Optimization)**: Policy gradient method with clipped surrogate objective\n",
    "- **A3C (Asynchronous Actor-Critic)**: Actor-critic with parallel environments\n",
    "\n",
    "## References\n",
    "- Mnih, V. et al. \"Human-level control through deep reinforcement learning.\" Nature, 2015.\n",
    "- Schulman, J. et al. \"Proximal Policy Optimization Algorithms.\" arXiv, 2017.\n",
    "- Mnih, V. et al. \"Asynchronous Methods for Deep Reinforcement Learning.\" ICML, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import asyncio\n",
    "import websockets\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "FINSIM_API_BASE = \"http://localhost:8000/api/v1\"\n",
    "AGENTS_API_BASE = \"http://localhost:8001/api/v1\"\n",
    "WEBSOCKET_URL = \"ws://localhost:8000/ws/market\"\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinSimEnvironment:\n",
    "    \"\"\"Interface to FinSim simulation environment\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url=FINSIM_API_BASE):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "    def get_market_data(self, symbol):\n",
    "        \"\"\"Get current market data for symbol\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.base_url}/quotes/{symbol}\")\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting market data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def place_order(self, symbol, side, quantity, price=None):\n",
    "        \"\"\"Place order in simulation\"\"\"\n",
    "        try:\n",
    "            order_data = {\n",
    "                \"symbol\": symbol,\n",
    "                \"side\": side,\n",
    "                \"order_type\": \"limit\" if price else \"market\",\n",
    "                \"quantity\": quantity,\n",
    "                \"price\": price\n",
    "            }\n",
    "            \n",
    "            response = self.session.post(f\"{self.base_url}/orders\", json=order_data)\n",
    "            if response.status_code in [200, 201]:\n",
    "                return response.json()\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error placing order: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_portfolio(self):\n",
    "        \"\"\"Get current portfolio status\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.base_url}/portfolio\")\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            return {\"cash\": 100000, \"positions\": {}}\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting portfolio: {e}\")\n",
    "            return {\"cash\": 100000, \"positions\": {}}\n",
    "\n",
    "# Initialize environment\n",
    "env = FinSimEnvironment()\n",
    "print(\"FinSim environment initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep Q-Network (DQN) Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNetwork(nn.Module):\n",
    "    \"\"\"Deep Q-Network for trading decisions\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size=20, action_size=3, hidden_size=128):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, action_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent with Experience Replay\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size=20, action_size=3, lr=0.001, gamma=0.95, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        # Neural networks\n",
    "        self.q_network = DQNNetwork(state_size, action_size)\n",
    "        self.target_network = DQNNetwork(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        # Experience replay\n",
    "        self.memory = []\n",
    "        self.memory_size = 10000\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.total_reward = 0\n",
    "        self.episode_rewards = []\n",
    "        self.losses = []\n",
    "        \n",
    "        # Update target network\n",
    "        self.update_target_network()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from main network to target network\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay buffer\"\"\"\n",
    "        if len(self.memory) >= self.memory_size:\n",
    "            self.memory.pop(0)\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"Train the network on a batch of experiences\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample batch\n",
    "        batch = np.random.choice(len(self.memory), self.batch_size, replace=False)\n",
    "        states = torch.FloatTensor([self.memory[i][0] for i in batch])\n",
    "        actions = torch.LongTensor([self.memory[i][1] for i in batch])\n",
    "        rewards = torch.FloatTensor([self.memory[i][2] for i in batch])\n",
    "        next_states = torch.FloatTensor([self.memory[i][3] for i in batch])\n",
    "        dones = torch.BoolTensor([self.memory[i][4] for i in batch])\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Next Q values from target network\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.losses.append(loss.item())\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Initialize DQN agent\n",
    "dqn_agent = DQNAgent()\n",
    "print(\"DQN Agent initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PPO (Proximal Policy Optimization) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPONetwork(nn.Module):\n",
    "    \"\"\"PPO Actor-Critic Network\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size=20, action_size=3, hidden_size=128):\n",
    "        super(PPONetwork, self).__init__()\n",
    "        \n",
    "        # Shared layers\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Actor head (policy)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, action_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Critic head (value function)\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shared = self.shared_layers(x)\n",
    "        action_probs = self.actor(shared)\n",
    "        value = self.critic(shared)\n",
    "        return action_probs, value\n",
    "\n",
    "class PPOAgent:\n",
    "    \"\"\"PPO Agent with Clipped Surrogate Objective\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size=20, action_size=3, lr=0.0003, gamma=0.99, eps_clip=0.2, k_epochs=4):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.k_epochs = k_epochs\n",
    "        \n",
    "        # Networks\n",
    "        self.policy = PPONetwork(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        # Memory for trajectory\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.total_reward = 0\n",
    "        self.episode_rewards = []\n",
    "        self.losses = []\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"Select action using current policy\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action_probs, value = self.policy(state_tensor)\n",
    "            \n",
    "            # Sample action from policy\n",
    "            action_dist = torch.distributions.Categorical(action_probs)\n",
    "            action = action_dist.sample()\n",
    "            log_prob = action_dist.log_prob(action)\n",
    "            \n",
    "            return action.item(), log_prob.item(), value.item()\n",
    "    \n",
    "    def remember(self, state, action, reward, log_prob, value, done):\n",
    "        \"\"\"Store trajectory step\"\"\"\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def compute_returns_and_advantages(self):\n",
    "        \"\"\"Compute discounted returns and advantages\"\"\"\n",
    "        returns = []\n",
    "        advantages = []\n",
    "        \n",
    "        # Compute returns\n",
    "        discounted_reward = 0\n",
    "        for reward, done in zip(reversed(self.rewards), reversed(self.dones)):\n",
    "            if done:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + self.gamma * discounted_reward\n",
    "            returns.insert(0, discounted_reward)\n",
    "        \n",
    "        # Compute advantages\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        values = torch.tensor(self.values, dtype=torch.float32)\n",
    "        advantages = returns - values\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        return returns, advantages\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update policy using PPO\"\"\"\n",
    "        if not self.states:\n",
    "            return\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(np.array(self.states))\n",
    "        actions = torch.LongTensor(self.actions)\n",
    "        old_log_probs = torch.FloatTensor(self.log_probs)\n",
    "        \n",
    "        returns, advantages = self.compute_returns_and_advantages()\n",
    "        \n",
    "        # PPO update\n",
    "        for _ in range(self.k_epochs):\n",
    "            # Get current policy outputs\n",
    "            action_probs, values = self.policy(states)\n",
    "            action_dist = torch.distributions.Categorical(action_probs)\n",
    "            new_log_probs = action_dist.log_prob(actions)\n",
    "            entropy = action_dist.entropy().mean()\n",
    "            \n",
    "            # Compute ratio\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            \n",
    "            # Compute surrogate loss\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # Critic loss\n",
    "            critic_loss = F.mse_loss(values.squeeze(), returns)\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy\n",
    "            \n",
    "            # Update\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            self.losses.append(total_loss.item())\n",
    "        \n",
    "        # Clear memory\n",
    "        self.clear_memory()\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear trajectory memory\"\"\"\n",
    "        self.states.clear()\n",
    "        self.actions.clear()\n",
    "        self.rewards.clear()\n",
    "        self.log_probs.clear()\n",
    "        self.values.clear()\n",
    "        self.dones.clear()\n",
    "\n",
    "# Initialize PPO agent\n",
    "ppo_agent = PPOAgent()\n",
    "print(\"PPO Agent initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trading Environment State Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnvironment:\n",
    "    \"\"\"Trading environment for RL agents\"\"\"\n",
    "    \n",
    "    def __init__(self, symbols=['AAPL', 'GOOGL', 'MSFT'], initial_balance=100000):\n",
    "        self.symbols = symbols\n",
    "        self.initial_balance = initial_balance\n",
    "        self.balance = initial_balance\n",
    "        self.positions = {symbol: 0 for symbol in symbols}\n",
    "        self.price_history = {symbol: [] for symbol in symbols}\n",
    "        self.current_prices = {symbol: 100.0 for symbol in symbols}  # Initial prices\n",
    "        self.step_count = 0\n",
    "        self.transaction_cost = 0.001  # 0.1% transaction cost\n",
    "        \n",
    "        # Generate synthetic price data for demo\n",
    "        self.generate_price_data()\n",
    "    \n",
    "    def generate_price_data(self, n_steps=1000):\n",
    "        \"\"\"Generate synthetic price data for demonstration\"\"\"\n",
    "        for symbol in self.symbols:\n",
    "            # Generate correlated price movements\n",
    "            base_price = np.random.uniform(50, 200)\n",
    "            returns = np.random.normal(0.0005, 0.02, n_steps)  # Daily returns\n",
    "            prices = [base_price]\n",
    "            \n",
    "            for ret in returns:\n",
    "                new_price = prices[-1] * (1 + ret)\n",
    "                prices.append(max(new_price, 1.0))  # Prevent negative prices\n",
    "            \n",
    "            self.price_history[symbol] = prices\n",
    "            self.current_prices[symbol] = prices[0]\n",
    "    \n",
    "    def get_state(self, symbol, lookback=20):\n",
    "        \"\"\"Get current state representation for a symbol\"\"\"\n",
    "        if self.step_count < lookback:\n",
    "            # Pad with zeros for initial steps\n",
    "            prices = [self.current_prices[symbol]] * lookback\n",
    "        else:\n",
    "            start_idx = max(0, self.step_count - lookback + 1)\n",
    "            end_idx = self.step_count + 1\n",
    "            prices = self.price_history[symbol][start_idx:end_idx]\n",
    "            \n",
    "            if len(prices) < lookback:\n",
    "                prices = [prices[0]] * (lookback - len(prices)) + prices\n",
    "        \n",
    "        # Normalize prices\n",
    "        prices = np.array(prices)\n",
    "        normalized_prices = prices / prices[0] if prices[0] > 0 else prices\n",
    "        \n",
    "        # Calculate technical indicators\n",
    "        returns = np.diff(normalized_prices) if len(normalized_prices) > 1 else [0]\n",
    "        moving_avg = np.mean(normalized_prices[-5:]) if len(normalized_prices) >= 5 else normalized_prices[-1]\n",
    "        volatility = np.std(returns) if len(returns) > 1 else 0\n",
    "        \n",
    "        # Portfolio features\n",
    "        position_ratio = self.positions[symbol] / 1000  # Normalize position\n",
    "        balance_ratio = self.balance / self.initial_balance\n",
    "        \n",
    "        # Combine features\n",
    "        state = np.concatenate([\n",
    "            normalized_prices[-10:],  # Last 10 normalized prices\n",
    "            returns[-5:] if len(returns) >= 5 else [0] * 5,  # Last 5 returns\n",
    "            [moving_avg, volatility, position_ratio, balance_ratio, self.step_count / 1000]  # Additional features\n",
    "        ])\n",
    "        \n",
    "        # Ensure fixed size\n",
    "        if len(state) < 20:\n",
    "            state = np.pad(state, (0, 20 - len(state)), 'constant')\n",
    "        elif len(state) > 20:\n",
    "            state = state[:20]\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, symbol, action):\n",
    "        \"\"\"Execute action and return reward\"\"\"\n",
    "        # Update price\n",
    "        if self.step_count < len(self.price_history[symbol]) - 1:\n",
    "            self.current_prices[symbol] = self.price_history[symbol][self.step_count + 1]\n",
    "        \n",
    "        current_price = self.current_prices[symbol]\n",
    "        quantity = 10  # Fixed quantity for simplicity\n",
    "        \n",
    "        # Calculate reward based on action\n",
    "        reward = 0\n",
    "        \n",
    "        if action == 1:  # Buy\n",
    "            cost = quantity * current_price * (1 + self.transaction_cost)\n",
    "            if self.balance >= cost:\n",
    "                self.balance -= cost\n",
    "                self.positions[symbol] += quantity\n",
    "                reward = -self.transaction_cost  # Small penalty for transaction cost\n",
    "        \n",
    "        elif action == 2:  # Sell\n",
    "            if self.positions[symbol] >= quantity:\n",
    "                proceeds = quantity * current_price * (1 - self.transaction_cost)\n",
    "                self.balance += proceeds\n",
    "                self.positions[symbol] -= quantity\n",
    "                reward = -self.transaction_cost  # Small penalty for transaction cost\n",
    "        \n",
    "        # Add reward based on portfolio performance\n",
    "        if self.step_count > 0:\n",
    "            prev_value = self.get_portfolio_value(self.price_history[symbol][self.step_count])\n",
    "            curr_value = self.get_portfolio_value(current_price)\n",
    "            reward += (curr_value - prev_value) / prev_value if prev_value > 0 else 0\n",
    "        \n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = self.step_count >= len(self.price_history[symbol]) - 1\n",
    "        \n",
    "        return self.get_state(symbol), reward, done\n",
    "    \n",
    "    def get_portfolio_value(self, price):\n",
    "        \"\"\"Calculate total portfolio value\"\"\"\n",
    "        total_value = self.balance\n",
    "        for symbol in self.symbols:\n",
    "            total_value += self.positions[symbol] * price\n",
    "        return total_value\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment\"\"\"\n",
    "        self.balance = self.initial_balance\n",
    "        self.positions = {symbol: 0 for symbol in self.symbols}\n",
    "        self.step_count = 0\n",
    "        for symbol in self.symbols:\n",
    "            self.current_prices[symbol] = self.price_history[symbol][0]\n",
    "        return self.get_state(self.symbols[0])\n",
    "\n",
    "# Initialize trading environment\n",
    "trading_env = TradingEnvironment()\n",
    "print(\"Trading environment initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Live Trading Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trading_episode(agent, agent_name, symbol='AAPL', episodes=10):\n",
    "    \"\"\"Run trading episodes with an RL agent\"\"\"\n",
    "    print(f\"\\n=== Running {agent_name} Trading Simulation ===\")\n",
    "    \n",
    "    episode_rewards = []\n",
    "    portfolio_values = []\n",
    "    actions_taken = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = trading_env.reset()\n",
    "        total_reward = 0\n",
    "        actions = []\n",
    "        values = []\n",
    "        \n",
    "        while True:\n",
    "            # Get action from agent\n",
    "            if hasattr(agent, 'act'):\n",
    "                if agent_name == 'PPO':\n",
    "                    action, log_prob, value = agent.act(state)\n",
    "                else:\n",
    "                    action = agent.act(state)\n",
    "            else:\n",
    "                action = np.random.randint(0, 3)  # Random baseline\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, done = trading_env.step(symbol, action)\n",
    "            \n",
    "            # Store experience\n",
    "            if hasattr(agent, 'remember'):\n",
    "                if agent_name == 'DQN':\n",
    "                    agent.remember(state, action, reward, next_state, done)\n",
    "                elif agent_name == 'PPO':\n",
    "                    agent.remember(state, action, reward, log_prob, value, done)\n",
    "            \n",
    "            # Track metrics\n",
    "            total_reward += reward\n",
    "            actions.append(action)\n",
    "            values.append(trading_env.get_portfolio_value(trading_env.current_prices[symbol]))\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Update agent\n",
    "        if hasattr(agent, 'replay') and agent_name == 'DQN':\n",
    "            if episode % 5 == 0:  # Train every 5 episodes\n",
    "                for _ in range(10):\n",
    "                    agent.replay()\n",
    "                agent.update_target_network()\n",
    "        \n",
    "        elif hasattr(agent, 'update') and agent_name == 'PPO':\n",
    "            agent.update()\n",
    "        \n",
    "        # Record episode results\n",
    "        episode_rewards.append(total_reward)\n",
    "        portfolio_values.append(values)\n",
    "        actions_taken.append(actions)\n",
    "        \n",
    "        if episode % 2 == 0:\n",
    "            final_value = trading_env.get_portfolio_value(trading_env.current_prices[symbol])\n",
    "            print(f\"Episode {episode + 1}: Reward={total_reward:.4f}, Portfolio Value=${final_value:.2f}, Actions={len(set(actions))} unique\")\n",
    "    \n",
    "    return episode_rewards, portfolio_values, actions_taken\n",
    "\n",
    "# Run simulations\n",
    "print(\"Starting RL agent trading simulations...\")\n",
    "\n",
    "# DQN simulation\n",
    "dqn_rewards, dqn_values, dqn_actions = run_trading_episode(dqn_agent, 'DQN', episodes=10)\n",
    "\n",
    "# PPO simulation  \n",
    "ppo_rewards, ppo_values, ppo_actions = run_trading_episode(ppo_agent, 'PPO', episodes=10)\n",
    "\n",
    "# Random baseline\n",
    "random_rewards, random_values, random_actions = run_trading_episode(None, 'Random', episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('RL Agent Trading Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Episode rewards comparison\n",
    "axes[0, 0].plot(range(1, 11), dqn_rewards, 'b-o', label='DQN', linewidth=2)\n",
    "axes[0, 0].plot(range(1, 11), ppo_rewards, 'r-s', label='PPO', linewidth=2)\n",
    "axes[0, 0].plot(range(1, 11), random_rewards, 'g--^', label='Random', linewidth=2)\n",
    "axes[0, 0].set_title('Episode Rewards', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Total Reward')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Portfolio value evolution (last episode)\n",
    "if dqn_values and ppo_values and random_values:\n",
    "    axes[0, 1].plot(dqn_values[-1], 'b-', label='DQN', linewidth=2)\n",
    "    axes[0, 1].plot(ppo_values[-1], 'r-', label='PPO', linewidth=2)\n",
    "    axes[0, 1].plot(random_values[-1], 'g--', label='Random', linewidth=2)\n",
    "    axes[0, 1].axhline(y=100000, color='black', linestyle=':', label='Initial Value')\n",
    "    axes[0, 1].set_title('Portfolio Value Evolution (Last Episode)', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Time Steps')\n",
    "    axes[0, 1].set_ylabel('Portfolio Value ($)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Action distribution (last episode)\n",
    "action_names = ['Hold', 'Buy', 'Sell']\n",
    "if dqn_actions:\n",
    "    dqn_action_counts = [dqn_actions[-1].count(i) for i in range(3)]\n",
    "    ppo_action_counts = [ppo_actions[-1].count(i) for i in range(3)]\n",
    "    random_action_counts = [random_actions[-1].count(i) for i in range(3)]\n",
    "    \n",
    "    x = np.arange(len(action_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    axes[0, 2].bar(x - width, dqn_action_counts, width, label='DQN', alpha=0.8)\n",
    "    axes[0, 2].bar(x, ppo_action_counts, width, label='PPO', alpha=0.8)\n",
    "    axes[0, 2].bar(x + width, random_action_counts, width, label='Random', alpha=0.8)\n",
    "    \n",
    "    axes[0, 2].set_title('Action Distribution (Last Episode)', fontweight='bold')\n",
    "    axes[0, 2].set_xlabel('Actions')\n",
    "    axes[0, 2].set_ylabel('Frequency')\n",
    "    axes[0, 2].set_xticks(x)\n",
    "    axes[0, 2].set_xticklabels(action_names)\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning curves\n",
    "if hasattr(dqn_agent, 'losses') and dqn_agent.losses:\n",
    "    axes[1, 0].plot(dqn_agent.losses, 'b-', alpha=0.7)\n",
    "    axes[1, 0].set_title('DQN Training Loss', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Training Step')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'No DQN training data', ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "    axes[1, 0].set_title('DQN Training Loss', fontweight='bold')\n",
    "\n",
    "if hasattr(ppo_agent, 'losses') and ppo_agent.losses:\n",
    "    axes[1, 1].plot(ppo_agent.losses, 'r-', alpha=0.7)\n",
    "    axes[1, 1].set_title('PPO Training Loss', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Training Step')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'No PPO training data', ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "    axes[1, 1].set_title('PPO Training Loss', fontweight='bold')\n",
    "\n",
    "# Performance metrics summary\n",
    "metrics_data = {\n",
    "    'Agent': ['DQN', 'PPO', 'Random'],\n",
    "    'Avg Reward': [np.mean(dqn_rewards), np.mean(ppo_rewards), np.mean(random_rewards)],\n",
    "    'Final Portfolio': [\n",
    "        dqn_values[-1][-1] if dqn_values else 100000,\n",
    "        ppo_values[-1][-1] if ppo_values else 100000,\n",
    "        random_values[-1][-1] if random_values else 100000\n",
    "    ],\n",
    "    'Reward Std': [np.std(dqn_rewards), np.std(ppo_rewards), np.std(random_rewards)]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "print(\"\\n=== Performance Summary ===\")\n",
    "print(metrics_df.round(2))\n",
    "\n",
    "# Create a summary table plot\n",
    "axes[1, 2].axis('tight')\n",
    "axes[1, 2].axis('off')\n",
    "table = axes[1, 2].table(cellText=metrics_df.round(2).values,\n",
    "                        colLabels=metrics_df.columns,\n",
    "                        cellLoc='center',\n",
    "                        loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.5)\n",
    "axes[1, 2].set_title('Performance Metrics', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional analysis\n",
    "print(\"\\n=== Additional Analysis ===\")\n",
    "print(f\"DQN - Best Episode Reward: {max(dqn_rewards):.4f}\")\n",
    "print(f\"PPO - Best Episode Reward: {max(ppo_rewards):.4f}\")\n",
    "print(f\"Random - Best Episode Reward: {max(random_rewards):.4f}\")\n",
    "\n",
    "if hasattr(dqn_agent, 'epsilon'):\n",
    "    print(f\"DQN Final Epsilon: {dqn_agent.epsilon:.4f}\")\n",
    "\n",
    "print(\"\\nSimulation completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Connect to Live FinSim Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and deploy RL agents to FinSim\n",
    "import requests\n",
    "\n",
    "def deploy_rl_agent_to_finsim(agent_type, symbols=['AAPL'], parameters=None):\n",
    "    \"\"\"Deploy RL agent to FinSim agents service\"\"\"\n",
    "    if parameters is None:\n",
    "        parameters = {}\n",
    "    \n",
    "    agent_config = {\n",
    "        \"agent_id\": f\"rl_{agent_type}_{int(time.time())}\",\n",
    "        \"agent_type\": agent_type.lower(),\n",
    "        \"symbols\": symbols,\n",
    "        \"parameters\": parameters,\n",
    "        \"enabled\": True\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{AGENTS_API_BASE}/agents\",\n",
    "            json=agent_config,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        if response.status_code in [200, 201]:\n",
    "            result = response.json()\n",
    "            print(f\"✅ Successfully deployed {agent_type} agent: {result['agent_id']}\")\n",
    "            return result['agent_id']\n",
    "        else:\n",
    "            print(f\"❌ Failed to deploy {agent_type} agent: {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error deploying {agent_type} agent: {e}\")\n",
    "        return None\n",
    "\n",
    "# Deploy different RL agents\n",
    "print(\"Deploying RL agents to FinSim...\")\n",
    "\n",
    "# Deploy DQN agent\n",
    "dqn_id = deploy_rl_agent_to_finsim(\n",
    "    \"DQN\",\n",
    "    symbols=['AAPL', 'GOOGL'],\n",
    "    parameters={'epsilon': 0.1, 'learning_rate': 0.001}\n",
    ")\n",
    "\n",
    "# Deploy PPO agent\n",
    "ppo_id = deploy_rl_agent_to_finsim(\n",
    "    \"PPO\",\n",
    "    symbols=['MSFT', 'TSLA'],\n",
    "    parameters={'epsilon': 0.2, 'gamma': 0.99}\n",
    ")\n",
    "\n",
    "# Deploy A3C agent\n",
    "a3c_id = deploy_rl_agent_to_finsim(\n",
    "    \"A3C\",\n",
    "    symbols=['NVDA'],\n",
    "    parameters={'gamma': 0.95, 'learning_rate': 0.0001}\n",
    ")\n",
    "\n",
    "print(\"\\n=== Deployed RL Agents ===\")\n",
    "if dqn_id:\n",
    "    print(f\"DQN Agent ID: {dqn_id}\")\n",
    "if ppo_id:\n",
    "    print(f\"PPO Agent ID: {ppo_id}\")\n",
    "if a3c_id:\n",
    "    print(f\"A3C Agent ID: {a3c_id}\")\n",
    "\n",
    "print(\"\\nRL agents are now live trading in the FinSim environment!\")\n",
    "print(\"Monitor their performance through the FinSim dashboard.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Real-time Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_agent_performance(agent_ids, duration=60):\n",
    "    \"\"\"Monitor RL agent performance in real-time\"\"\"\n",
    "    print(f\"Monitoring agent performance for {duration} seconds...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    performance_data = {agent_id: [] for agent_id in agent_ids}\n",
    "    \n",
    "    while time.time() - start_time < duration:\n",
    "        for agent_id in agent_ids:\n",
    "            try:\n",
    "                response = requests.get(\n",
    "                    f\"{AGENTS_API_BASE}/agents/{agent_id}\",\n",
    "                    timeout=5\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    performance_data[agent_id].append({\n",
    "                        'timestamp': datetime.now(),\n",
    "                        'pnl': data.get('performance', {}).get('total_pnl', 0),\n",
    "                        'trades': data.get('performance', {}).get('total_trades', 0),\n",
    "                        'status': data.get('status', 'unknown')\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error monitoring agent {agent_id}: {e}\")\n",
    "        \n",
    "        time.sleep(5)  # Update every 5 seconds\n",
    "        \n",
    "        # Clear output and show current status\n",
    "        print(f\"\\rTime elapsed: {int(time.time() - start_time)}s\", end='')\n",
    "    \n",
    "    print(\"\\n\\n=== Final Performance Summary ===\")\n",
    "    for agent_id, data in performance_data.items():\n",
    "        if data:\n",
    "            final_data = data[-1]\n",
    "            print(f\"{agent_id}: PnL=${final_data['pnl']:.2f}, Trades={final_data['trades']}, Status={final_data['status']}\")\n",
    "        else:\n",
    "            print(f\"{agent_id}: No data received\")\n",
    "    \n",
    "    return performance_data\n",
    "\n",
    "# Monitor deployed agents\n",
    "deployed_agents = [agent_id for agent_id in [dqn_id, ppo_id, a3c_id] if agent_id]\n",
    "\n",
    "if deployed_agents:\n",
    "    print(\"Starting real-time monitoring...\")\n",
    "    performance_data = monitor_agent_performance(deployed_agents, duration=30)\n",
    "else:\n",
    "    print(\"No agents deployed for monitoring.\")\n",
    "\n",
    "print(\"\\n🎉 RL Agent Demo Completed Successfully!\")\n",
    "print(\"\\nKey Achievements:\")\n",
    "print(\"✅ Implemented DQN, PPO, and A3C agents\")\n",
    "print(\"✅ Demonstrated live trading in simulated environment\")\n",
    "print(\"✅ Deployed agents to FinSim platform\")\n",
    "print(\"✅ Real-time performance monitoring\")\n",
    "print(\"\\nThe RL agents are now running autonomously in the FinSim environment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}